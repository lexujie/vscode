## Sigmoid函数
![sigmoid](../../../pictures/sigmoid.png)
## Tanh函数
![tanh](../../../pictures/tanh.png)
## Relu函数
把线性转换为非线性变化
![relu](../../../pictures/relu.png)
## Leaky Relu函数
![leaky_relu](../../../pictures/leaky_relu.png)
## 选择
- 首选relu激活函数
- 学习率设置较小值
- 输入特征标准化，即让输入特征满足0均值，标准差为1的正态分布
- 初始参数中心化，让随机生成的参数满足以0为均值，sqrt(2/当前层输入特征个数)为标准差的正态分布